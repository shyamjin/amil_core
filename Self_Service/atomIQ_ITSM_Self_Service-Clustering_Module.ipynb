{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atomIQ Ticketing Self Service - Clustering Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basic packages - found in the Anaconda release\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.externals import joblib\n",
    "import logging\n",
    "import logging.config\n",
    "from time import time, gmtime, strftime\n",
    "from datetime import datetime\n",
    "import re\n",
    "from scipy.sparse import vstack, hstack\n",
    "import json\n",
    "import sys\n",
    "from PIL import Image\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import json\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matplotlib imports\n",
    "\n",
    "#%matplotlib notebook \n",
    "#Magic command to view plots in Jupyter notebooks. disable when running as the application\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') #Configures matplotlib for the application server and does not look for a GUI\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Amily_Path=\"/prjvl01/amily/Self_Service/\"\n",
    "Output_Path=\"/UTSAmilyAttachments/AMILY_TO_UTS/\"\n",
    "ERROR_MSG_FOR_USER = \"An error has occured while clustering data on atomIQ ticketing, the operation has been aborted\"\n",
    "LABELS_ONLY=True #True if only label data is provided by UTS, and not textual fields\n",
    "GMM_COMPONENTS=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Configure Self Service Log\n",
    "logging.basicConfig(filename=Amily_Path+'Logs/self_service.log',\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s', \n",
    "                    datefmt='%Y-%m-%d,%H:%M:%S',\n",
    "                    level=logging.DEBUG\n",
    "                   )\n",
    "\n",
    "#Disable DEBUG loggings from PIL library\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"requests\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Enviroment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identification of running enviroment - UAT or Prod\n",
    "import socket\n",
    "server=socket.gethostname()\n",
    "UAT=True\n",
    "if 'prd3' in server:\n",
    "    UAT=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unit Test indicator\n",
    "TEST_ENVIROMENT=True if sys.argv[1]=='-f' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Endpoints for AO integration - Load from configuration file\n",
    "with open(Amily_Path+'Features/Configurations/ao_endpoints.json') as json_data:\n",
    "    endpoints = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amily Reply REST Call to UTS-AO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def send_result_request(success, detailed_results_path=None, stats_report_path=None, \n",
    "                        error_message=None, company='Unknown', request_id=0, Ack=False):\n",
    "    if TEST_ENVIROMENT:\n",
    "        return\n",
    "    \n",
    "    comapny_name=company.replace(\"_\",\" \")\n",
    "    \n",
    "    #Login request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/login'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/login'\n",
    "    url=endpoints[server]['login']\n",
    "    headers = {'Content-Type' : 'application/json'}\n",
    "    body=str('{\"username\":\"Amily\",\\n\"password\":\"12345678\"}')\n",
    "    login_request = requests.post(url\n",
    "                                  ,data=body\n",
    "                                  ,headers=headers\n",
    "                                  ,verify = False\n",
    "                                 )\n",
    "    try:\n",
    "        token=login_request.headers['Authentication-Token']\n",
    "    except:\n",
    "        logging.error('Failed to fetch AO Token for REST Service')\n",
    "        return\n",
    "\n",
    "    \n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    url=endpoints[server]['ack']\n",
    "    headers = {'Content-Type' : 'application/json' ,'Authentication-Token': token}\n",
    "    \n",
    "    #File paths request\n",
    "    if not Ack:\n",
    "        #file_paths_dict = r\"\"\"{\"inputParameters\":[{\"name\":\"Inputs\",\"value\":\"{'XmlFields': [{'FilePath': '/tmp/test.txt'},{'Operation': 'report'}]}\"}]}\"\"\"\n",
    "        fp1=r\"\"\"{\"inputParameters\": [{\"name\":\"Inputs\",\"value\":\"{'XmlFields': [{'Field': [{'FilePath': '\"\"\"\n",
    "        fp2=r\"\"\"'},{'Operation':'report'}]},{'Field': [{'FilePath':'\"\"\"\n",
    "        fp3=r\"\"\"'},{'Operation':'stats'}]},{'Field': [{'Status':'\"\"\"\n",
    "        fp4=r\"\"\"'},{'StatusDescription':'\"\"\"\n",
    "        fp5=r\"\"\"'},{'Company':'\"\"\"\n",
    "        fp6=r\"\"\"'},{'RequestID': '\"\"\"\n",
    "        fp7=r\"\"\"'}]}]}\"}]}\"\"\"\n",
    "\n",
    "        if not error_message:\n",
    "            error_message=\"Operation successful\"\n",
    "        status=\"Success\" if success else \"Failure\"\n",
    "\n",
    "        file_paths_dict=''.join([fp1,str(detailed_results_path),fp2,str(stats_report_path),fp3,status,fp4,str(error_message),fp5,str(comapny_name),fp6,str(request_id),fp7])\n",
    "        \n",
    "        result_request = requests.post(url \n",
    "                                       ,data=file_paths_dict\n",
    "                                       ,headers=headers\n",
    "                                       ,verify = False\n",
    "                                      )\n",
    "\n",
    "        #print(file_paths_dict)\n",
    "        if result_request.status_code!=200:\n",
    "            logging.error('Failed in sending file paths to AO')\n",
    "        else:\n",
    "            logging.info('Reply from UTS - >'+result_request.text)\n",
    "            \n",
    "    #Acknowledgement request\n",
    "    if Ack:\n",
    "        fp1=r\"\"\"{\"inputParameters\":[{\"name\":\"Inputs\",\"value\":\"{'Ack': [{'Field': [{'Status': 'InProgress'},{'StatusDescription': 'Operation successful'},{'Company': '\"\"\"\n",
    "        fp2=r\"\"\"'},{'RequestID': '\"\"\"\n",
    "        fp3=r\"\"\"'}]}]}\"}]}\"\"\"\n",
    "        ack_dict=\"\".join([fp1,str(comapny_name),fp2,str(request_id),fp3])\n",
    "        result_request = requests.post(url \n",
    "                                       ,data=ack_dict\n",
    "                                       ,headers=headers\n",
    "                                       ,verify = False\n",
    "                                      )\n",
    "        #print(file_paths_dict)\n",
    "        if result_request.status_code!=200:\n",
    "            logging.error('Failed in sending acknowledgement to AO')\n",
    "        else:\n",
    "            logging.info('Acknowledgement Reply from UTS - >'+result_request.text)\n",
    "        \n",
    "    \n",
    "    #Logout request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/logout'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/logout'\n",
    "    url=endpoints[server]['logout']\n",
    "    headers = {'Content-Type' : 'application/json','Authentication-Token': token}\n",
    "    logout_request = requests.post(url\n",
    "                                   ,headers=headers\n",
    "                                   ,verify = False\n",
    "                                  )\n",
    "    if logout_request.status_code!=200:\n",
    "        logging.error('Failed logging out from AO')\n",
    "        return\n",
    "    else:\n",
    "        logging.info('Successfully logged out from AO')\n",
    "    \n",
    "    if not Ack: logging.info('File paths sent successfully to UTS')\n",
    "    #print('\\nsuccess :-)')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DB read\n",
    "def read_corpus(path, cols):\n",
    "    \n",
    "    full_df = pd.read_csv(path, delimiter='\\t', encoding=\"utf8\")\n",
    "    \n",
    "    #Set column names to predefined values\n",
    "    column_dict = {}\n",
    "    column_dict[cols['ticket_id']]='ticket_id'\n",
    "    if not LABELS_ONLY:\n",
    "        for idx, textual_field in enumerate(cols['textual_fields']):\n",
    "            col_name=str(\"_\".join(['textual_field',str(idx+1)]))\n",
    "            column_dict[textual_field]=col_name\n",
    "    for idx, filter_field in enumerate(cols['filter_fields']):\n",
    "        col_name=str(\"_\".join(['filter_field',str(idx+1)]))\n",
    "        column_dict[filter_field]=col_name\n",
    "    #column_dict[cols['label_field']]='label'\n",
    "    \n",
    "    full_df.rename(columns=column_dict,inplace=True)\n",
    "    \n",
    "    #Filter filed by filtered values - Unneccesary in Current UTS implementation - The data will already be filtered\n",
    "    '''\n",
    "    for idx, filter_field in enumerate(cols['filter_fields']):\n",
    "        filter_field_column=str(\"_\".join(['filter_field',str(idx+1)]))\n",
    "        full_df=full_df.loc[full_df[filter_field_column]==cols['filter_values'][idx]]\n",
    "    '''\n",
    "        \n",
    "    #Drop NA only after filters were done and only for the relevant columns\n",
    "    column_list=[]\n",
    "    for key, value in column_dict.items():\n",
    "        column_list.append(value)\n",
    "    #full_df.dropna(inplace=True, subset=column_list)\n",
    "    full_df = full_df.drop_duplicates(subset=['ticket_id']).sort_values(by=['ticket_id']).reset_index(drop=True) \n",
    "    #print(full_df.info())\n",
    "    \n",
    "    return full_df[column_list]\n",
    "    #Return a shuffled-row-order data frame as a preperation for the cross validation\n",
    "    #return full_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Archive files - copy to Archive Directory and remove from orignal directory\n",
    "def archive_infile(infile_name):\n",
    "    try:\n",
    "        file_name=infile_name[infile_name.rfind('/')+1:]\n",
    "        copyfile(infile_name, Amily_Path+\"Archive/\"+file_name)\n",
    "        os.remove(infile_name)\n",
    "        logging.info('Clustering training file was moved to Archive folder')\n",
    "    except:\n",
    "        logging.warning('Clustering training file was not moved succesffuly to Archive folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - FILE WILL BE RECEIVED FROM ITSM\n",
    "try:\n",
    "    if TEST_ENVIROMENT:\n",
    "        infile_name = Amily_Path+\"Unit-test/Data/Unit1--000000000000948.txt\" #Internal use only - testing purposes\n",
    "    else:\n",
    "        infile_name = str(sys.argv[1])\n",
    "\n",
    "    logging.info('----------------- CLUSTERING SESSION HAS STARTED-----------------')\n",
    "    logging.info(\"File was loaded for clustering: \"+infile_name)\n",
    "except:\n",
    "    logging.error(\"Could not open file. OPERATION ABORTED\")\n",
    "    send_result_request(success=False, error_message='atomIQ ticketing could not read the file sent by UTS. Operation Aborted')\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - DEFAULT PARAMETERS FOR UTS\n",
    "account_name=infile_name[infile_name.rfind('/')+1:infile_name.rfind(\"--\")]\n",
    "request_id=infile_name[infile_name.rfind(\"--\")+2:infile_name.rfind(\".\")]\n",
    "ticket_id_field = \"Incident Number\"\n",
    "textual_fields=[\"DESCRIPTION\",\"DETAILED_DECRIPTION\"]\n",
    "filter_fields=[\"Origin Type\"]\n",
    "#filter_values=[\"Yes\"]\n",
    "label_field = \"Label\"\n",
    "generate_pickles=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "if LABELS_ONLY:\n",
    "    cols = {\"ticket_id\":ticket_id_field\n",
    "           ,\"filter_fields\":filter_fields\n",
    "           #,\"filter_values\":filter_values\n",
    "           #,\"label_field\":label_field\n",
    "           }\n",
    "else:\n",
    "    cols = {\"ticket_id\":ticket_id_field\n",
    "           ,\"textual_fields\":textual_fields\n",
    "           ,\"filter_fields\":filter_fields\n",
    "           #,\"filter_values\":filter_values\n",
    "           #,\"label_field\":label_field\n",
    "           }\n",
    "\n",
    "try:\n",
    "    train_df = read_corpus(infile_name,cols)\n",
    "    \n",
    "    filter_values=[train_df[\"filter_field_1\"].unique()[0]] #Is External Yes or No - A very UTS-specific implementation\n",
    "    train_df=train_df.loc[train_df[\"filter_field_1\"]==filter_values[0]].reset_index(drop=True) #Make sure using only one filter value field\n",
    "    \n",
    "    send_result_request(success=True, company=account_name, request_id=request_id, Ack=True)\n",
    "    logging.info(\"Data for clustering - %s account, %d unique tickets, Is_External=%s\"%(account_name,train_df.shape[0],filter_values[0]))\n",
    "    if train_df[\"filter_field_1\"].unique().shape[0]>1:\n",
    "        logging.warning('More than 1 filter field values')\n",
    "except:\n",
    "    logging.error(\"Unable to load file. OPERATION ABORTED\")\n",
    "    #send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Load From File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sparse Matrix from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file_prefix = \"_\".join([account_name.replace(\" \",\"_\"),str(filter_fields[0])+'-'+str(filter_values[0])]) \n",
    "    file_name=\".\".join([file_prefix,'npz'])\n",
    "\n",
    "    #A workaround for the following command as scipy save_npz does not work on scipy 0.18.1 version -> train_feat_loaded = load_npz('./Features/'+file_name)\n",
    "    loaded_npz = np.load(Amily_Path+'Features/'+file_name)\n",
    "    train_feat_loaded = loaded_npz[loaded_npz.keys()[0]].item()\n",
    "    del(loaded_npz)\n",
    "    logging.info('%s was succesfully uploaded from disk'%file_name)\n",
    "except:\n",
    "    logging.error('%s failed to load from disk. OPERATION ABORTED'%file_name)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Account Configuration file containing mapping of ticket IDs and textual fields feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file_prefix = \"_\".join([account_name.replace(\" \",\"_\"),str(filter_fields[0])+'-'+str(filter_values[0])]) \n",
    "    file_name=\".\".join([file_prefix,'json'])\n",
    "    account_dict = json.load(open(Amily_Path+'Features/'+file_name))\n",
    "    transformed_textual_lengths_loaded=account_dict['text_limits']\n",
    "    transformed_ticket_ids=account_dict['ticket_ids']\n",
    "    logging.info('%s textual configuration file succesfully uploaded from disk'%account_name)\n",
    "except:\n",
    "    logging.error('%s textual configuration file failed to load from disk. OPERATION ABORTED'%account_name)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Loaded Data to existing tickets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The assumption is that both data sources - transformed features and train data from user - are SORTED by ticket ID\n",
    "#This is enforced by the algorithm\n",
    "try:\n",
    "    transformed_ticket_ids_df=pd.DataFrame(transformed_ticket_ids, columns=['ticket_id_trans'])\n",
    "    train_ticket_ids_df=pd.DataFrame(train_df['ticket_id'].tolist(), columns=['ticket_id_train'])\n",
    "    #Left join all tickets ids that are transformed with all ticket ids received by the user\n",
    "    comparison_df=pd.merge(transformed_ticket_ids_df, train_ticket_ids_df, how='left', \n",
    "                           left_on=['ticket_id_trans'],right_on=['ticket_id_train'])\n",
    "\n",
    "    missing_tickets = train_df['ticket_id'].shape[0]- comparison_df['ticket_id_train'].count()\n",
    "    #print('Number of tickets in train set with transformations found - ',comparison_df['ticket_id_train'].count())\n",
    "    if missing_tickets>0:\n",
    "        logging.info('Number of tickets in train set with features transformations not found - %d'%missing_tickets)\n",
    "        #Extract a list of tickets passed by the user and were not found in feature trnasformation matrix\n",
    "        missing_df = pd.merge(train_ticket_ids_df, comparison_df, how='left', \n",
    "                              left_on=['ticket_id_train'],right_on=['ticket_id_train'])\n",
    "        removed_tickets_from_train=missing_df.loc[missing_df['ticket_id_trans'].isnull()][['ticket_id_train']]['ticket_id_train'].tolist()\n",
    "        #print(removed_tickets_from_train) #for debugging purposes\n",
    "        #Remove unfound tickets from training data frame\n",
    "        train_df=train_df[~train_df['ticket_id'].isin(removed_tickets_from_train)].reset_index(drop=True)\n",
    "\n",
    "    indices = np.where(comparison_df['ticket_id_trans']==comparison_df['ticket_id_train'])[0]\n",
    "    train_feat_trans = train_feat_loaded[indices,:]\n",
    "    logging.info('Merge of loaded textual features and data in file completed succesfully')\n",
    "except:\n",
    "    logging.error('Merge of loaded textual features and data in file failed. OPERATION ABORTED')\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dimensionality Reduction using TruncatedSVD'''\n",
    "def svd_dim_reduction(train_feat, components, verbose=False):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    start=time()\n",
    "    if verbose:\n",
    "        print('Dimensionality redcuction started at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    svd = TruncatedSVD(n_components=components)\n",
    "    svd.fit(train_feat)\n",
    "    train_feat_SVD = svd.fit_transform(train_feat,y=None)\n",
    "    if verbose:\n",
    "        print('Dimensionality redcuction concluded successfully at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    logging.info('Dimensionality reduction of data completed succesfully. Time to complete - %1.2f[min.]'%((time()-start)/60))\n",
    "    return train_feat_SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_clustering(train_db,num_components,verbose=False):\n",
    "    from sklearn import mixture\n",
    "    \n",
    "    if verbose:\n",
    "        print('GMM Clustering started at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    \n",
    "    clf = mixture.GaussianMixture(n_components=num_components, covariance_type='full')\n",
    "    clf.fit(train_db)\n",
    "    db=clf.predict(train_db)\n",
    "    \n",
    "    if verbose:\n",
    "        print('GMM Clustering concluded successfully at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbscan_clustering (train_db, epsilon, minimum_cluster_size, verbose=False):\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    if verbose:\n",
    "        print('DBSCAN Clustering started at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    \n",
    "    #train_df normalization for cosine metric\n",
    "    Xnorm = np.linalg.norm(train_db,axis = 1)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    Xnormed = np.divide(train_db,Xnorm.reshape(Xnorm.shape[0],1))\n",
    "    Xnormed = np.nan_to_num(Xnormed)\n",
    "    \n",
    "    db = DBSCAN(eps = epsilon, min_samples = minimum_cluster_size, metric = 'euclidean', n_jobs=-1).fit_predict(Xnormed)\n",
    "    \n",
    "    if verbose:\n",
    "        print('DBSCAN Clustering concluded successfully at ',end ='')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Clusters\n",
    "def generate_clusters(svd_components=50, dbscan_eps=0.6, dbscan_min_cluster_size=10, two_phase_dbscan = False,\n",
    "                     num_components=GMM_COMPONENTS):\n",
    "    verbose=True if TEST_ENVIROMENT else False\n",
    "    start=time()\n",
    "    try:\n",
    "        train_feat_dim_reduced = svd_dim_reduction(train_feat_trans, svd_components, verbose=verbose)\n",
    "        #train_df['cluster'] = dbscan_clustering(train_feat_dim_reduced, dbscan_eps, dbscan_min_cluster_size, verbose=verbose)\n",
    "        train_df['cluster']=gmm_clustering(train_feat_dim_reduced,num_components,verbose=verbose)\n",
    "\n",
    "        clustered_df = pd.DataFrame(train_feat_dim_reduced)\n",
    "        clustered_df['ticket_id'] = train_df['ticket_id']\n",
    "        clustered_df['cluster'] = train_df['cluster']\n",
    "\n",
    "        clustered_df['cluster'] = clustered_df['cluster']+1\n",
    "    \n",
    "        logging.info('Clustering completed succesfully. Time to complete - %1.2f[min.]'%((time()-start)/60))\n",
    "        return clustered_df\n",
    "    except:\n",
    "        logging.error('Could not complete clustering process. OPERATION ABORTED')\n",
    "        send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "        return\n",
    "\n",
    "clustered_df=generate_clusters()\n",
    "if not isinstance(clustered_df, pd.DataFrame):\n",
    "    if not TEST_ENVIROMENT: \n",
    "        archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ticket_id_field=\"Incident Number\"\n",
    "    \n",
    "    if not TEST_ENVIROMENT:\n",
    "        detailed_report_path=Output_Path+account_name+'--'+request_id+\".txt\"\n",
    "    else:\n",
    "        detailed_report_path = Amily_Path+\"Unit-test/Output/\"+account_name+'--'+request_id+\".txt\"\n",
    "\n",
    "    feedback_df = pd.DataFrame(clustered_df['ticket_id'])\n",
    "    #feedback_df['Recommended_Label']=np.NaN\n",
    "    #feedback_df['Probability']=np.NaN\n",
    "    feedback_df['Cluster']=clustered_df['cluster']\n",
    "    \n",
    "    feedback_df=feedback_df.rename(columns={\"ticket_id\": ticket_id_field})\n",
    "    \n",
    "    if missing_tickets>0:\n",
    "        tickets_with_no_transformation_df = pd.DataFrame(removed_tickets_from_train, columns=[ticket_id_field])\n",
    "        tickets_with_no_transformation_df['Cluster']=np.nan\n",
    "        feedback_df = feedback_df.append(tickets_with_no_transformation_df, ignore_index=True)\n",
    "    feedback_df['Cluster'] = feedback_df['Cluster'].dropna().apply(lambda x: str(int(x)))\n",
    "    \n",
    "    feedback_df.to_csv(detailed_report_path,index=None,sep=\",\")\n",
    "    logging.info('%s was created succesfully'%detailed_report_path)\n",
    "    #del(feedback_df)\n",
    "except:\n",
    "    logging.error('%s was not created succesfully. OPERATION ABORTED'%detailed_report_path)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    #exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For each cluster fetch one representative example \n",
    "def fetch_cluster_example_recommendation(clustered_df, cluster_ind, two_phase_dbscan = False):\n",
    "    filtered_df = clustered_df.loc[clustered_df.cluster == cluster_ind, :].reset_index(drop=True)\n",
    "    size_of_cluster = filtered_df.shape[0] \n",
    "    \n",
    "    #2 phase DBSCAN - Currently Disabled\n",
    "    if two_phase_dbscan:\n",
    "        mean_distance_between_pairs = filtered_df.iloc[:,:-5].mean().mean(axis = 0) \n",
    "        filtered_df['cluster_minor'] = dbscan_clustering(filtered_df.iloc[:,:-4], abs(mean_distance_between_pairs*10), int(filtered_df.shape[0]/4))\n",
    "        largest_cluster_ind = np.argmax(filtered_df.cluster_minor.value_counts()) \n",
    "        filtered_df = filtered_df.loc[filtered_df.cluster_minor == largest_cluster_ind, :].reset_index(drop=True)\n",
    "        #print(filtered_df)\n",
    "\n",
    "    #Compute pairwise distance matrix\n",
    "    distance_matrix = pd.DataFrame(euclidean_distances(filtered_df.iloc[:,:-5],filtered_df.iloc[:,:-5]))\n",
    "    distance_matrix['average'] = distance_matrix.mean(axis = 0)\n",
    "    index_min = distance_matrix.average.idxmin(axis = 1)\n",
    "\n",
    "    ticket_id = filtered_df.ticket_id.iloc[index_min]\n",
    "    size_of_cluster_print = '{:d} ({:.2f}% of training data)'.format(size_of_cluster,100*(size_of_cluster/clustered_df.shape[0]))\n",
    "    return [cluster_ind, size_of_cluster_print, ticket_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Clustering Summary Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_clustering_summary(recommendation_df):\n",
    "    from pandas.tools.plotting import table\n",
    "    fig, ax = plt.subplots(figsize=(10, recommendation_df.shape[0]*0.4)) # set size frame\n",
    "\n",
    "    ax.xaxis.set_visible(False)  # hide the x axis\n",
    "    ax.yaxis.set_visible(False)  # hide the y axis\n",
    "    ax.set_frame_on(False)  # no visible frame, uncomment if size is ok\n",
    "    #tabla = table(ax, stat, loc='upper right', colWidths=[0.12,0.12,0.18])  # where df is your data frame\n",
    "\n",
    "    tabla = table(ax, recommendation_df, loc='upper left',colWidths=[0.1,0.35,0.2])  # where df is your data frame\n",
    "\n",
    "    tabla.auto_set_font_size(False) # Activate set fontsize manually\n",
    "    tabla.set_fontsize(11) # if ++fontsize is necessary ++colWidths\n",
    "    tabla.scale(1.2, 1.2)\n",
    "\n",
    "    filter_value_dict={1:\"External\",0:\"Internal\"}\n",
    "    plt.figtext(0.24, 0.95, account_name.replace(\"_\",\" \")+\" - \"+filter_value_dict[filter_values[0]]+\" Tickets\"\n",
    "                     ,fontsize=14,fontweight='bold')\n",
    "    plt.figtext(0.12, 0.9,\"atomIQ Ticketing Clustering Results - \"+strftime(\"%Y-%m-%d %H:%M\")+\" (CMI TZ)\"\n",
    "                     ,fontsize=12,fontweight='bold')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create report and exoprt the report to file\n",
    "summary_plot_success=True\n",
    "try:\n",
    "    recommendation_list = []\n",
    "    for i in range (np.max(clustered_df['cluster'])):\n",
    "        try:\n",
    "            reco = fetch_cluster_example_recommendation(clustered_df, i+1)\n",
    "            recommendation_list.append(reco)\n",
    "        except:\n",
    "            pass\n",
    "    recommendation_df=pd.DataFrame(recommendation_list,columns=['Cluster #','Cluster Size','Sample Ticket ID'])\n",
    "    #recommendation_df['Cluster #']=recommendation_df['Cluster #']+1\n",
    "    summary_plt = plot_clustering_summary(recommendation_df)\n",
    "    summary_plt.savefig(Amily_Path+'Images/clustering_report.jpg')\n",
    "except:\n",
    "    logging.error('Clustering summary report .jpg file was not created successfully')\n",
    "    summary_plot_success=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append Images to one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appends images to one image\n",
    "def pil_grid(images, max_horiz=np.iinfo(int).max):\n",
    "    n_images = len(images)\n",
    "    n_horiz = min(n_images, max_horiz)\n",
    "    h_sizes, v_sizes = [0] * n_horiz, [0] * (n_images // n_horiz)\n",
    "    for i, im in enumerate(images):\n",
    "        h, v = i % n_horiz, i // n_horiz\n",
    "        h_sizes[h] = max(h_sizes[h], im.size[0])\n",
    "        v_sizes[v] = max(v_sizes[v], im.size[1])\n",
    "    h_sizes, v_sizes = np.cumsum([0] + h_sizes), np.cumsum([0] + v_sizes)\n",
    "    im_grid = Image.new('RGB', (h_sizes[-1], v_sizes[-1]), color='white')\n",
    "    for i, im in enumerate(images):\n",
    "        im_grid.paste(im, (h_sizes[i % n_horiz], v_sizes[i // n_horiz]))\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add pictures to user feedback report, append them to one image and export to directory\n",
    "try:\n",
    "    list_im=[Amily_Path+\"Images/Fixed/atomiq_logo.png\"]\n",
    "    if summary_plot_success:\n",
    "        list_im.append(Amily_Path+\"Images/clustering_report.jpg\")\n",
    "    else:\n",
    "        list_im.append(Amily_Path+\"Images/Fixed/CM-Error.PNG\")\n",
    "\n",
    "    imgs = [Image.open(i) for i in list_im]\n",
    "    im = pil_grid(imgs,1)\n",
    "    if not TEST_ENVIROMENT:\n",
    "        saved_path=Output_Path+account_name+'--'+request_id+'.png'\n",
    "        im.save(saved_path)\n",
    "    else:\n",
    "        saved_path=Amily_Path+'Unit-test/Output/Training_Report_'+account_name+'--'+request_id+'.png'\n",
    "        im.save(saved_path)\n",
    "    logging.info('User clustering feedback report was genertaed successfully')\n",
    "except:\n",
    "    logging.error('User clustering feedback report was not genertaed successfully')\n",
    "    try:\n",
    "        #Send a textual file containing an error message\n",
    "        if not TEST_ENVIROMENT:\n",
    "            saved_path=Output_Path+'Training_Report_'+account_name+'--'+request_id+'.txt'\n",
    "        else:\n",
    "            saved_path=Amily_Path+'Unit-test/Output/Training_Report_'+account_name+'--'+request_id+'.txt'\n",
    "        with open(saved_path, 'w') as error_msg_file:\n",
    "            error_msg_file.write('An error occurred while generating the Auto labeling statistics report for '+account_name+' ['+strftime(\"%Y-%m-%d %H:%M\", gmtime())+']')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    os.remove(Amily_Path+'Images/clustering_report.jpg')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the detailed ticket feedback to a textual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "send_result_request(success=True, detailed_results_path=detailed_report_path, \n",
    "                    stats_report_path=saved_path, company=account_name, request_id=request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete training file from directory upon training completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not TEST_ENVIROMENT:\n",
    "    try:\n",
    "        archive_infile(infile_name)\n",
    "        logging.info('-----------------SUCCESSFULLY FINISHED CLUSTERING PROCESS FOR %s-----------------'%account_name)\n",
    "    except:\n",
    "        logging.error('Could not delete training file after completion of training')\n",
    "else:\n",
    "    logging.info('-----------------SUCCESSFULLY FINIHED CLUSTERING PROCESS FOR %s-----------------'%account_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
