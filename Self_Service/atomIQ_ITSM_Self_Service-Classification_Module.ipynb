{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atomIQ Ticketing Self Service - Classification Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basic packages - found in the Anaconda release\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.externals import joblib\n",
    "import logging\n",
    "import logging.config\n",
    "from time import time, gmtime, strftime\n",
    "import re\n",
    "from scipy.sparse import vstack, hstack\n",
    "import json\n",
    "import sys\n",
    "from PIL import Image\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matplotlib imports\n",
    "\n",
    "#%matplotlib notebook \n",
    "#Magic command to view plots in Jupyter notebooks. sidable when running as the application\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') #Configures matplotlib for the application server and does not look for a GUI\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Amily_Path=\"/prjvl01/amily/Self_Service/\"\n",
    "Output_Path=\"/UTSAmilyAttachments/AMILY_TO_UTS/\"\n",
    "ERROR_MSG_FOR_USER = \"An error has occured while training data on atomIQ ticketing, the operation has been aborted\"\n",
    "LABELS_ONLY=True #True if only label data is provided by UTS, and not textual fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Configure Self Service Log\n",
    "logging.basicConfig(filename=Amily_Path+'Logs/self_service.log',\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s', \n",
    "                    datefmt='%Y-%m-%d,%H:%M:%S',\n",
    "                    level=logging.DEBUG\n",
    "                   )\n",
    "\n",
    "#Disable DEBUG loggings from PIL library\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"requests\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Enviroment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Identification of running enviroment - UAT or Prod\n",
    "import socket\n",
    "server=socket.gethostname()\n",
    "UAT=True\n",
    "if 'prd3' in server:\n",
    "    UAT=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unit Test indicator\n",
    "TEST_ENVIROMENT=True if sys.argv[1]=='-f' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Endpoints for AO integration - Load from configuration file\n",
    "with open(Amily_Path+'Features/Configurations/ao_endpoints.json') as json_data:\n",
    "    endpoints = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deploy Model Is a parameter passed by the shell script activating the deployment module\n",
    "try:\n",
    "    DEPLOY_MODEL=True if sys.argv[2]==\"Deploy\" else False\n",
    "except:\n",
    "    DEPLOY_MODEL=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amily Reply REST Call to UTS-AO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def send_result_request(success, detailed_results_path=None, stats_report_path=None, \n",
    "                        error_message=None, company='Unknown', request_id=0, Ack=False):\n",
    "    if TEST_ENVIROMENT:\n",
    "        return\n",
    "    \n",
    "    comapny_name=company.replace(\"_\",\" \")\n",
    "    \n",
    "    #Login request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/login'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/login'\n",
    "    url=endpoints[server]['login']\n",
    "    headers = {'Content-Type' : 'application/json'}\n",
    "    body=str('{\"username\":\"Amily\",\\n\"password\":\"12345678\"}')\n",
    "    login_request = requests.post(url\n",
    "                                  ,data=body\n",
    "                                  ,headers=headers\n",
    "                                  ,verify = False\n",
    "                                 )\n",
    "    try:\n",
    "        token=login_request.headers['Authentication-Token']\n",
    "    except:\n",
    "        logging.error('Failed to fetch AO Token for REST Service')\n",
    "        return\n",
    "\n",
    "    \n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    url=endpoints[server]['ack']\n",
    "    headers = {'Content-Type' : 'application/json' ,'Authentication-Token': token}\n",
    "    \n",
    "    #File paths request\n",
    "    if not Ack:\n",
    "        #file_paths_dict = r\"\"\"{\"inputParameters\":[{\"name\":\"Inputs\",\"value\":\"{'XmlFields': [{'FilePath': '/tmp/test.txt'},{'Operation': 'report'}]}\"}]}\"\"\"\n",
    "        fp1=r\"\"\"{\"inputParameters\": [{\"name\":\"Inputs\",\"value\":\"{'XmlFields': [{'Field': [{'FilePath': '\"\"\"\n",
    "        fp2=r\"\"\"'},{'Operation':'report'}]},{'Field': [{'FilePath':'\"\"\"\n",
    "        fp3=r\"\"\"'},{'Operation':'stats'}]},{'Field': [{'Status':'\"\"\"\n",
    "        fp4=r\"\"\"'},{'StatusDescription':'\"\"\"\n",
    "        fp5=r\"\"\"'},{'Company':'\"\"\"\n",
    "        fp6=r\"\"\"'},{'RequestID': '\"\"\"\n",
    "        fp7=r\"\"\"'}]}]}\"}]}\"\"\"\n",
    "\n",
    "        if not error_message:\n",
    "            error_message=\"Operation successful\"\n",
    "        status=\"Success\" if success else \"Failure\"\n",
    "\n",
    "        file_paths_dict=''.join([fp1,str(detailed_results_path),fp2,str(stats_report_path),fp3,status,fp4,str(error_message),fp5,str(comapny_name),fp6,str(request_id),fp7])\n",
    "        \n",
    "        result_request = requests.post(url \n",
    "                                       ,data=file_paths_dict\n",
    "                                       ,headers=headers\n",
    "                                       ,verify = False\n",
    "                                      )\n",
    "\n",
    "        #print(file_paths_dict)\n",
    "        if result_request.status_code!=200:\n",
    "            logging.error('Failed in sending file paths to AO')\n",
    "        else:\n",
    "            logging.info('Reply from UTS - >'+result_request.text)\n",
    "            \n",
    "    #Acknowledgement request\n",
    "    if Ack:\n",
    "        fp1=r\"\"\"{\"inputParameters\":[{\"name\":\"Inputs\",\"value\":\"{'Ack': [{'Field': [{'Status': 'InProgress'},{'StatusDescription': 'Operation successful'},{'Company': '\"\"\"\n",
    "        fp2=r\"\"\"'},{'RequestID': '\"\"\"\n",
    "        fp3=r\"\"\"'}]}]}\"}]}\"\"\"\n",
    "        ack_dict=\"\".join([fp1,str(comapny_name),fp2,str(request_id),fp3])\n",
    "        result_request = requests.post(url \n",
    "                                       ,data=ack_dict\n",
    "                                       ,headers=headers\n",
    "                                       ,verify = False\n",
    "                                      )\n",
    "        #print(file_paths_dict)\n",
    "        if result_request.status_code!=200:\n",
    "            logging.error('Failed in sending acknowledgement to AO')\n",
    "        else:\n",
    "            logging.info('Acknowledgement Reply from UTS - >'+result_request.text)\n",
    "        \n",
    "    \n",
    "    #Logout request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/logout'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/logout'\n",
    "    url=endpoints[server]['logout']\n",
    "    headers = {'Content-Type' : 'application/json','Authentication-Token': token}\n",
    "    logout_request = requests.post(url\n",
    "                                   ,headers=headers\n",
    "                                   ,verify = False\n",
    "                                  )\n",
    "    if logout_request.status_code!=200:\n",
    "        logging.error('Failed logging out from AO')\n",
    "        return\n",
    "    else:\n",
    "        logging.info('Successfully logged out from AO')\n",
    "    \n",
    "    if not Ack: logging.info('File paths sent successfully to UTS')\n",
    "    #print('\\nsuccess :-)')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DB read\n",
    "def read_corpus(path, cols):\n",
    "    \n",
    "    full_df = pd.read_csv(path, delimiter='\\t', encoding=\"utf8\")\n",
    "    \n",
    "    #Set column names to predefined values\n",
    "    column_dict = {}\n",
    "    column_dict[cols['ticket_id']]='ticket_id'\n",
    "    if not LABELS_ONLY:\n",
    "        for idx, textual_field in enumerate(cols['textual_fields']):\n",
    "            col_name=str(\"_\".join(['textual_field',str(idx+1)]))\n",
    "            column_dict[textual_field]=col_name\n",
    "    for idx, filter_field in enumerate(cols['filter_fields']):\n",
    "        col_name=str(\"_\".join(['filter_field',str(idx+1)]))\n",
    "        column_dict[filter_field]=col_name\n",
    "    column_dict[cols['label_field']]='label'\n",
    "    \n",
    "    full_df.rename(columns=column_dict,inplace=True)\n",
    "    \n",
    "    #Filter filed by filtered values - Unneccesary in Current UTS implementation - The data will already be filtered\n",
    "    '''\n",
    "    for idx, filter_field in enumerate(cols['filter_fields']):\n",
    "        filter_field_column=str(\"_\".join(['filter_field',str(idx+1)]))\n",
    "        full_df=full_df.loc[full_df[filter_field_column]==cols['filter_values'][idx]]\n",
    "    '''\n",
    "    \n",
    "    #Fill blank labels with value \"Other\"\n",
    "    full_df['label']=full_df['label'].fillna(\"Other\")\n",
    "        \n",
    "    #Drop NA only after filters were done and only for the relevant columns\n",
    "    column_list=[]\n",
    "    for key, value in column_dict.items():\n",
    "        column_list.append(value)\n",
    "    #full_df.dropna(inplace=True, subset=column_list)\n",
    "    full_df = full_df.drop_duplicates(subset=['ticket_id']).sort_values(by=['ticket_id']).reset_index(drop=True) \n",
    "    #print(full_df.info())\n",
    "    \n",
    "    return full_df\n",
    "    #Return a shuffled-row-order data frame as a preperation for the cross validation\n",
    "    #return full_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Archive files - copy to Archive Directory and remove from orignal directory\n",
    "def archive_infile(infile_name):\n",
    "    try:\n",
    "        file_name=infile_name[infile_name.rfind('/')+1:]\n",
    "        copyfile(infile_name, Amily_Path+\"Archive/\"+file_name)\n",
    "        os.remove(infile_name)\n",
    "        logging.info('Training file was moved to Archive folder')\n",
    "    except:\n",
    "        logging.warning('Training file was not moved succesffuly to Archive folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - FILE WILL BE RECEIVED FROM ITSM\n",
    "try:\n",
    "    if TEST_ENVIROMENT:\n",
    "        infile_name = Amily_Path+\"Unit-test/Data/MetroPCS Communications, Inc.--000000000001315.txt\" #Internal use only - testing purposes\n",
    "    else:\n",
    "        infile_name = str(sys.argv[1])\n",
    "    QUICK_TRAINING=True\n",
    "    if infile_name.find('Full')>0:\n",
    "        QUICK_TRAINING=False\n",
    "\n",
    "    logging.info('-----------------%s TRAINING SESSION HAS STARTED-----------------'%('QUICK' if QUICK_TRAINING else 'FULL'))\n",
    "    logging.info(\"File was loaded for classification: \"+infile_name)\n",
    "except:\n",
    "    logging.error(\"Could not open file. OPERATION ABORTED\")\n",
    "    send_result_request(success=False, error_message='atomIQ ticketing could not read the file sent by UTS. Operation Aborted')\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - DEFAULT PARAMETERS FOR UTS\n",
    "account_name=infile_name[infile_name.rfind('/')+1:infile_name.rfind(\"--\")]\n",
    "request_id=infile_name[infile_name.rfind(\"--\")+2:infile_name.rfind(\".\")]\n",
    "ticket_id_field = \"Incident Number\"\n",
    "textual_fields=[\"DESCRIPTION\",\"DETAILED_DECRIPTION\"]\n",
    "filter_fields=[\"Origin Type\"]\n",
    "#filter_values=[\"Yes\"]\n",
    "label_field = \"Label\"\n",
    "generate_pickles=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "if LABELS_ONLY:\n",
    "    cols = {\"ticket_id\":ticket_id_field\n",
    "           ,\"filter_fields\":filter_fields\n",
    "           #,\"filter_values\":filter_values\n",
    "           ,\"label_field\":label_field}\n",
    "else:\n",
    "    cols = {\"ticket_id\":ticket_id_field\n",
    "           ,\"textual_fields\":textual_fields\n",
    "           ,\"filter_fields\":filter_fields\n",
    "           #,\"filter_values\":filter_values\n",
    "           ,\"label_field\":label_field}\n",
    "\n",
    "try:\n",
    "    train_df = read_corpus(infile_name,cols)\n",
    "    \n",
    "    filter_values=[train_df[\"filter_field_1\"].unique()[0]] #Is External Yes or No - A very UTS-specific implementation\n",
    "    train_df=train_df.loc[train_df[\"filter_field_1\"]==filter_values[0]].reset_index(drop=True) #Make sure using only one filter value field\n",
    "    \n",
    "    send_result_request(success=True, company=account_name, request_id=request_id, Ack=True)\n",
    "    logging.info(\"Data for training - %s account, %d unique tickets, Is_External=%s\"%(account_name,train_df.shape[0],filter_values[0]))\n",
    "    if train_df[\"filter_field_1\"].unique().shape[0]>1:\n",
    "        logging.warning('More than 1 filter field values')\n",
    "except:\n",
    "    logging.error(\"Unable to load file. OPERATION ABORTED\")\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Load From File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sparse Matrix from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file_prefix = \"_\".join([account_name.replace(\" \",\"_\"),str(filter_fields[0])+'-'+str(filter_values[0])]) \n",
    "    file_name=\".\".join([file_prefix,'npz'])\n",
    "\n",
    "    #A workaround for the following command as scipy save_npz does not work on scipy 0.18.1 version -> train_feat_loaded = load_npz('./Features/'+file_name)\n",
    "    loaded_npz = np.load(Amily_Path+'Features/'+file_name)\n",
    "    train_feat_loaded = loaded_npz[loaded_npz.keys()[0]].item()\n",
    "    del(loaded_npz)\n",
    "    logging.info('%s was succesfully uploaded from disk'%file_name)\n",
    "except:\n",
    "    logging.error('%s failed to load from disk. OPERATION ABORTED'%file_name)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Account Configuration file containing mapping of ticket IDs and textual fields feature lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file_prefix = \"_\".join([account_name.replace(\" \",\"_\"),str(filter_fields[0])+'-'+str(filter_values[0])]) \n",
    "    file_name=\".\".join([file_prefix,'json'])\n",
    "    account_dict = json.load(open(Amily_Path+'Features/'+file_name))\n",
    "    transformed_textual_lengths_loaded=account_dict['text_limits']\n",
    "    transformed_ticket_ids=account_dict['ticket_ids']\n",
    "    logging.info('%s textual configuration file succesfully uploaded from disk'%account_name)\n",
    "except:\n",
    "    logging.error('%s textual configuration file failed to load from disk. OPERATION ABORTED'%account_name)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Loaded Data to existing tickets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The assumption is that both data sources - transformed features and train data from user - are SORTED by ticket ID\n",
    "#This is enforced by the algorithm\n",
    "try:\n",
    "    transformed_ticket_ids_df=pd.DataFrame(transformed_ticket_ids, columns=['ticket_id_trans'])\n",
    "    train_ticket_ids_df=pd.DataFrame(train_df['ticket_id'].tolist(), columns=['ticket_id_train'])\n",
    "    #Left join all tickets ids that are transformed with all ticket ids received by the user\n",
    "    comparison_df=pd.merge(transformed_ticket_ids_df, train_ticket_ids_df, how='left', \n",
    "                           left_on=['ticket_id_trans'],right_on=['ticket_id_train'])\n",
    "\n",
    "    missing_tickets = train_df['ticket_id'].shape[0]- comparison_df['ticket_id_train'].count()\n",
    "    #print('Number of tickets in train set with transformations found - ',comparison_df['ticket_id_train'].count())\n",
    "    if missing_tickets>0:\n",
    "        logging.info('Number of tickets in train set with features transformations not found - %d'%missing_tickets)\n",
    "        #Extract a list of tickets passed by the user and were not found in feature trnasformation matrix\n",
    "        missing_df = pd.merge(train_ticket_ids_df, comparison_df, how='left', \n",
    "        left_on=['ticket_id_train'],right_on=['ticket_id_train'])\n",
    "        removed_tickets_from_train=missing_df.loc[missing_df['ticket_id_trans'].isnull()][['ticket_id_train']]['ticket_id_train'].tolist()\n",
    "        #print(removed_tickets_from_train) #for debugging purposes\n",
    "        #Remove unfound tickets from training data frame\n",
    "        train_df=train_df[~train_df['ticket_id'].isin(removed_tickets_from_train)].reset_index(drop=True)\n",
    "\n",
    "    indices = np.where(comparison_df['ticket_id_trans']==comparison_df['ticket_id_train'])[0]\n",
    "    train_feat_trans = train_feat_loaded[indices,:]\n",
    "    logging.info('Merge of loaded textual features and data in file completed succesfully')\n",
    "except:\n",
    "    logging.error('Merge of loaded textual features and data in file failed. OPERATION ABORTED')\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Data as a Prep. for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    index = np.arange(np.shape(train_feat_trans)[0])\n",
    "    np.random.shuffle(index)\n",
    "    train_feat_trans_shuffled=train_feat_trans[index, :] #Features Shuffled\n",
    "    label_series=train_df.label[index] #Labels Shuffled\n",
    "except:\n",
    "    logging.error('shuffling of train data as a prep. for CV has failed. OPERATION ABORTED')\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual fields Feature Weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A transformer which assigns weights to textual fields features\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class FeatureWeighting(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  transformer_weights,transformed_textual_lengths):\n",
    "        self.transformer_weights = transformer_weights\n",
    "        self.transformed_textual_lengths = transformed_textual_lengths\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        start=0\n",
    "        end=0\n",
    "        for i in range(len(self.transformer_weights)):\n",
    "            field_name=\"_\".join(['textual_field',str(i+1)])\n",
    "            start=end\n",
    "            end=end+self.transformed_textual_lengths[field_name]\n",
    "            added_matrix=X[:,start:end]*self.transformer_weights[field_name]\n",
    "            if i==0:\n",
    "                #print(start,end)\n",
    "                train_feat_stacked = added_matrix\n",
    "            else:\n",
    "                #print(start,end)\n",
    "                train_feat_stacked =hstack([train_feat_stacked, added_matrix] ,format='csr')\n",
    "                return train_feat_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifier Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "#from tqdm import tqdm\n",
    "\n",
    "def build_classification_model(X_train, y_train, transformed_textual_lengths, \n",
    "                               CV=True,\n",
    "                               Full_model=False, #True if all instances should be trained, regardless of cross validation\n",
    "                               Quick_training=False, #True if quick training should be applied - default parameters\n",
    "                               outpath=None, \n",
    "                               verbose=False, \n",
    "                               inner_CV=False #CV when training the CV folds\n",
    "                              ):\n",
    "\n",
    "    def default_account_parameter_values():\n",
    "    #Load default parameters for account and filter values for quick training\n",
    "        found = False\n",
    "        file_name = \"default_classification_parameters.json\" \n",
    "        param_dicts = json.load(open(Amily_Path+'Features/Configurations/'+file_name))\n",
    "        filter_value_dict={1:\"ext\",0:\"int\"}\n",
    "        for param_dict in param_dicts:\n",
    "            if param_dict['account']==account_name.replace(\" \",\"_\"):\n",
    "                param_dict_filter=\"_\".join([\"default_parameters\",filter_value_dict[filter_values[0]]])\n",
    "                param_dict_default=param_dict[param_dict_filter]\n",
    "                found = True\n",
    "                break\n",
    "        return param_dict_default, found\n",
    "\n",
    "    def build(classification_model, X, y=None, param_dict_default=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Define a modelling pipeline with SelectPercentile as feature selector\n",
    "        w_dict = {\"textual_field_1\":0.5,\"textual_field_2\":0.5}\n",
    "        classifier = Pipeline([(\"union\", FeatureWeighting(transformer_weights=w_dict,\n",
    "                                                          transformed_textual_lengths=transformed_textual_lengths)),\n",
    "                               (\"selector\", SelectPercentile(score_func=chi2))])\n",
    "        percentile_range = [20]\n",
    "        textual_weights_range=[\n",
    "                              #{\"textual_field_1\":0,\"textual_field_2\":1},\n",
    "                              {\"textual_field_1\":0.25,\"textual_field_2\":0.75}\n",
    "                              ,{\"textual_field_1\":0.5,\"textual_field_2\":0.5}\n",
    "                              ,{\"textual_field_1\":0.75,\"textual_field_2\":0.25}\n",
    "                              #,{\"textual_field_1\":1,\"textual_field_2\":0}\n",
    "                            ]\n",
    "        param_dict = {'selector__percentile': percentile_range\n",
    "                     ,'union__transformer_weights': textual_weights_range\n",
    "                     }\n",
    "        \n",
    "        #print(classifier.get_params().keys())\n",
    "        \n",
    "        #Model Option 1 - Random Forest\n",
    "        if classification_model == \"rf\":\n",
    "            classifier.steps.append([\"randforest\", RandomForestClassifier(class_weight=\"balanced\",n_jobs=-1)])\n",
    "            param_range=[100,200,500]\n",
    "            #param_range=[200]\n",
    "            param_dict['randforest__n_estimators']=param_range\n",
    "            if Quick_training:\n",
    "                param_dict=param_dict_default\n",
    "            model = GridSearchCV(estimator = classifier, param_grid=[param_dict], n_jobs=1,verbose=0)\n",
    "        \n",
    "        #Model Option 2 - SVM\n",
    "        '''\n",
    "        if classification_model == \"svm\":\n",
    "            classifier.steps.append([\"svm\", SVC(probability=True, random_state=1)])\n",
    "            #print(classifier.get_params().keys())\n",
    "            param_range = [0.0001, 0.001, 0.01, 0.01, 1.0, 10.0, 100.0, 1000.0]\n",
    "            param_grid = [{'selector__percentile': percentile_range,'svm__C': param_range,'svm__kernel':['linear']}\n",
    "                          ,\n",
    "                         {'selector__percentile': percentile_range,'svm__C': param_range,'svm__gamma': param_range,'svm__kernel':['rbf']}\n",
    "                         ]\n",
    "            model = GridSearchCV(estimator = classifier, param_grid=param_grid, n_jobs=-1, verbose=0)\n",
    "        '''\n",
    "        \n",
    "        #number of CV folds within outer folds\n",
    "        if inner_CV:\n",
    "            model.cv = 5\n",
    "        else:\n",
    "            model.cv = [(slice(None),slice(None))]\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    cross_validation = 5\n",
    "    test_results=[]\n",
    "    labels = LabelEncoder().fit(y_train)\n",
    "    num_classes=len(labels.classes_.tolist())\n",
    "    \n",
    "    if Quick_training:\n",
    "        param_dict_default, found_default_for_account=default_account_parameter_values()\n",
    "        if not found_default_for_account:\n",
    "            logging.error(\"Could not find default training parameters for account %s. OPERATION ABORTED\"%account_name)\n",
    "            exit()\n",
    "    else:\n",
    "        param_dict_default=None\n",
    "    \n",
    "    if CV:\n",
    "        step = train_df.shape[0]//cross_validation\n",
    "        last_step_addition = train_df.shape[0]%cross_validation\n",
    "        prob_df_success = True\n",
    "        for i in range(cross_validation):\n",
    "            X_test_cv={}\n",
    "            X_train_cv={}\n",
    "        \n",
    "            #Split train and test uniformly so all tickets would be a part of a test set and evaluated. Data was was already shuffled\n",
    "            if i == (cross_validation-1):\n",
    "                X_test_cv=X_train[i*step:((i+1)*step+last_step_addition)]\n",
    "                X_train_cv=X_train[:i*step]\n",
    "                y_test_cv=y_train[i*step:((i+1)*step+last_step_addition)]\n",
    "                y_train_cv=y_train[:i*step]   \n",
    "            else:\n",
    "                X_test_cv=X_train[i*step:(i+1)*step]\n",
    "                y_test_cv=y_train[i*step:(i+1)*step]\n",
    "                if i==0:\n",
    "                    X_train_cv=X_train[(i+1)*step:]\n",
    "                    y_train_cv=y_train[(i+1)*step:]\n",
    "                else:\n",
    "                    X_train_cv=vstack([X_train[:i*step],X_train[(i+1)*step:]])\n",
    "                    y_train_cv=y_train.loc[~y_train.index.isin(y_test_cv.index)]\n",
    "            \n",
    "                \n",
    "            #labels = LabelEncoder()\n",
    "            y_test_cv_encoded = labels.transform(y_test_cv)\n",
    "            y_train_cv_encoded = labels.transform(y_train_cv)\n",
    "\n",
    "            if verbose: print(\"Building for evaluation - Fold %d/%d\" %(i+1,cross_validation))\n",
    "\n",
    "            model = build(\"rf\", X_train_cv, y_train_cv_encoded,param_dict_default)\n",
    "            model.labels_=labels\n",
    "\n",
    "            y_pred = model.predict(X_test_cv)\n",
    "            test_pred_prob = model.predict_proba(X_test_cv)\n",
    "\n",
    "            #Add to predcited flow (+prob) of test matrix - will be used for the user feedback\n",
    "            test_results.append(list(zip(y_test_cv.index,labels.inverse_transform(y_pred),np.max(test_pred_prob, axis=1))))\n",
    "            \n",
    "            #Add to full probabilities of test matrix - will be used for the thresholds sensitivity analysis\n",
    "            try:\n",
    "                if np.unique(y_train_cv_encoded).shape[0]==num_classes:\n",
    "                    try:\n",
    "                        #Concatenate if already exists\n",
    "                        #print(i,'--->',np.unique(y_train_cv_encoded).shape[0])\n",
    "                        full_test_results_prob=np.vstack([full_test_results_prob,test_pred_prob])\n",
    "                    except:\n",
    "                        #Create a new probabilities matrix\n",
    "                        full_test_results_prob=test_pred_prob\n",
    "            except:\n",
    "                prob_df_success = False\n",
    "            \n",
    "            if verbose:\n",
    "                print(model.best_params_)\n",
    "                \n",
    "        #Generate the full predicted probabilities dataframe - will be used for the thresholds sensitivity analysis\n",
    "        if prob_df_success:\n",
    "            try:\n",
    "                prob_df = pd.DataFrame(full_test_results_prob,\n",
    "                                       columns=labels.inverse_transform(np.arange(full_test_results_prob.shape[1])))\n",
    "                prob_df['label']=y_train.reset_index(drop=True)\n",
    "            except:\n",
    "                prob_df=None\n",
    "        else:\n",
    "            prob_df=None\n",
    "    \n",
    "    if Full_model:\n",
    "        if verbose: print(\"Building model over all training data\")\n",
    "        y_train_encoded = labels.transform(y_train)\n",
    "        model = build(\"rf\", X_train, y_train_encoded,param_dict_default)\n",
    "        prob_df=None\n",
    "    \n",
    "    model.labels_=labels\n",
    "    return model, test_results, prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start=time()\n",
    "Quick_training=(QUICK_TRAINING and not DEPLOY_MODEL)\n",
    "verbose=True if TEST_ENVIROMENT else False\n",
    "\n",
    "model, test_results, prob_df = build_classification_model(train_feat_trans_shuffled\n",
    "                                                 ,label_series\n",
    "                                                 ,transformed_textual_lengths_loaded\n",
    "                                                 ,CV=not DEPLOY_MODEL\n",
    "                                                 ,Full_model=DEPLOY_MODEL\n",
    "                                                 ,Quick_training=Quick_training\n",
    "                                                 ,verbose=verbose\n",
    "                                                )\n",
    "logging.info('Training of data completed succesfully. Time to complete training - %1.2f[min.]'%((time()-start)/60))\n",
    "#Log best parameters if not Quick Training\n",
    "if not Quick_training: \n",
    "    BP = \"\"\n",
    "    for key, value in model.best_params_.items():\n",
    "        BP += str(', %s: %s'%(key,value))\n",
    "    logging.info('Best Parameters in training:'+BP[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Classification Model\n",
    "try:\n",
    "    start=time()\n",
    "    Quick_training=(QUICK_TRAINING and not DEPLOY_MODEL)\n",
    "    verbose=True if TEST_ENVIROMENT else False\n",
    "    \n",
    "    model, test_results, prob_df = build_classification_model(train_feat_trans_shuffled\n",
    "                                                     ,label_series\n",
    "                                                     ,transformed_textual_lengths_loaded\n",
    "                                                     ,CV=not DEPLOY_MODEL\n",
    "                                                     ,Full_model=DEPLOY_MODEL\n",
    "                                                     ,Quick_training=Quick_training\n",
    "                                                     ,verbose=verbose\n",
    "                                                    )\n",
    "    logging.info('Training of data completed succesfully. Time to complete training - %1.2f[min.]'%((time()-start)/60))\n",
    "    #Log best parameters if not Quick Training\n",
    "    if not Quick_training: \n",
    "        BP = \"\"\n",
    "        for key, value in model.best_params_.items():\n",
    "            BP += str(', %s: %s'%(key,value))\n",
    "        logging.info('Best Parameters in training:'+BP[1:])\n",
    "except:\n",
    "    logging.error('Training of data failed. OPERATION ABORTED')\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate User Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#produce precision and recall statistics + user labels-atomIQ labels mismathces dataframe\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "def generate_user_feedback(test_results,feedback_threshold = 0.3, filter_mismatches=False):\n",
    "    #Flatten Result list as the basis for the feedback to the user\n",
    "    test_results_flattened = [item for sublist in test_results for item in sublist]\n",
    "    #Convert list of test results to pandas data frame\n",
    "    feedback_df = pd.DataFrame(test_results_flattened, columns=['index','label-atomIQ','prob']).sort_values(by=['index'])\n",
    "    #Join ticket ID and user label with model results\n",
    "    feedback_df = pd.merge(train_df[['ticket_id','label']].reset_index(), feedback_df, how='left', on=['index'])\n",
    "    feedback_df=feedback_df.sort_values(by=['index'])\n",
    "    \n",
    "    #Produce precision and recall stats\n",
    "    stat_df = feedback_df.dropna().drop_duplicates(subset=['index','label','label-atomIQ'])\n",
    "    statistical_results = clsr(stat_df['label'], stat_df['label-atomIQ'])\n",
    "    \n",
    "    \n",
    "    #Filter results where model and user label mismatch\n",
    "    filtered_df = feedback_df.loc[(feedback_df['label-atomIQ']!=feedback_df['label'])].dropna()\n",
    "    #Filter data where probability is above feedback threshold \n",
    "    filtered_df = filtered_df.loc[((filtered_df['prob']>feedback_threshold) & (filtered_df['label-atomIQ']!=\"Other\"))\n",
    "               | ((filtered_df['prob']>1-feedback_threshold) & (filtered_df['label-atomIQ']==\"Other\"))]\n",
    "    #In case of duplicate indeces keep the lowest probability (favors FNs)\n",
    "    filtered_df=filtered_df.sort_values(by=['index','prob']).drop_duplicates(subset=['index']\n",
    "                                                                             #,keep='last'\n",
    "                                                                            )\n",
    "    if filter_mismatches:\n",
    "        return statistical_results, filtered_df\n",
    "    else:\n",
    "        return statistical_results, feedback_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if not DEPLOY_MODEL:\n",
    "        stats, feedback_df = generate_user_feedback(test_results,\n",
    "                                                    #filter_mismatches=True\n",
    "                                                   )\n",
    "        logging.info('User feedback was generated succesfully')\n",
    "except:\n",
    "    logging.error('User feedback was not generated succesfully. OPERATION ABORTED')\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the detailed ticket feedback to a textual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if not DEPLOY_MODEL:\n",
    "        if not TEST_ENVIROMENT:\n",
    "            detailed_report_path=Output_Path+account_name+'--'+request_id+\".txt\"\n",
    "        else:\n",
    "            detailed_report_path = Amily_Path+\"Unit-test/Output/\"+account_name+'--'+request_id+\".txt\"\n",
    "        \n",
    "        feedback_df['cluster']=None #UTS implementation - for a unified file format regardless of model (classification/clustering)\n",
    "        feedback_df = feedback_df.rename(columns={'ticket_id': 'Incident_Number', 'label-atomIQ': 'Recommended_Label',\n",
    "                                              'prob':'Probability','cluster':'Cluster'})\n",
    "        \n",
    "        #detailed_report_df=feedback_df.loc[feedback_df['Recommended_Label']!=\"Other\"] #Do not export \"Other\" results to UTS\n",
    "        #detailed_report_df[['Incident_Number','Recommended_Label','Probability','Cluster']].to_csv(detailed_report_path,index=None,sep=\",\")\n",
    "        \n",
    "        feedback_df[['Incident_Number','Recommended_Label','Probability','Cluster']].to_csv(detailed_report_path,index=None,sep=\",\")\n",
    "        \n",
    "        logging.info('%s was created succesfully'%detailed_report_path)\n",
    "        #del(detailed_report_df)\n",
    "except:\n",
    "    logging.error('%s was not created succesfully. OPERATION ABORTED'%detailed_report_path)\n",
    "    send_result_request(success=False, error_message=ERROR_MSG_FOR_USER, company=account_name, request_id=request_id)\n",
    "    if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the full probabilities matrix for future thresholds sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not DEPLOY_MODEL:\n",
    "    try:\n",
    "        prob_df.to_csv(Amily_Path+'Archive/Classification/'+account_name+'--'+request_id+\".txt\",index=None,sep=\",\")\n",
    "        logging.info('Full classification probabilities matrix for threshold sensitiviy analysis was archived succesfully')\n",
    "    except:\n",
    "        logging.warning('Full classification probabilities matrix for threshold sensitiviy analysis was not archived succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the classification report to PNG file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate a Confusion Matrix Plot\n",
    "def plot_confusion_matrix(cm_masked, cm, classes\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    #fig,axs=plt.subplots(2,1)\n",
    "    \n",
    "    max_length_of_label_text=len(max(classes, key=len)) #cm.shape[0]*0.6\n",
    "    fig = plt.figure(frameon=False, figsize=(9,9)) #adaptive figure size causes errors\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=colors.ListedColormap(['#c6efce']),)\n",
    "\n",
    "    cmap = colors.ListedColormap(['#fffef9','#fffddd', '#f2e8ff'])\n",
    "    bounds=[0,1,max(int(0.1*np.max(cm_masked)),5),max(np.max(cm_masked),10)]\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    plt.imshow(cm_masked, interpolation='nearest', cmap=cmap,norm=norm)\n",
    "    \n",
    "    filter_value_dict={1:\"External\",0:\"Internal\"}\n",
    "    plt.suptitle(account_name.replace(\"_\",\" \")+\" - \"+filter_value_dict[filter_values[0]]+\" Tickets\\natomIQ Ticketing Auto Labeling Results - \"+strftime(\"%Y-%m-%d %H:%M\")+\" (CMI TZ)\"\n",
    "                 , y=0.99, fontsize=14,fontweight='bold')\n",
    "    plt.title('\\n\\n\\n\\nAuto labeling compared to my original labeling',fontsize=10,fontweight='bold')\n",
    "    \n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"#d3d3d3\" if cm[i,j]==0 else \"black\",fontsize=120/cm.shape[0])\n",
    "    \n",
    "    plt.ylabel('My label'\n",
    "               ,color='#686e77'\n",
    "               ,fontweight='bold')\n",
    "    plt.xlabel('\\nPredicted label by atomIQ ticketing'\n",
    "               ,color='#686e77'\n",
    "               ,fontweight='bold')\n",
    "    #plt.figtext(.0, .05, \"IMPORTANT NOTE:\\nA ticket is assigned\\nto a flow if the predicted\\nproability is above 0.5.\\nThreshold modification may\\noffset misclassfications\")\n",
    "    #plt.colorbar(shrink=0.5,fontsize=5)\n",
    "    \n",
    "    #ColorBar\n",
    "    cb = plt.colorbar(shrink=0.5)\n",
    "    cb.ax.set_yticklabels(cb.ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Excepts a 2D array representing the confusion matrix and returns a 2D array where \"Other\" is pushed to the end \n",
    "#+ the ordered list of flow names for plotting\n",
    "def move_Other_to_end_of_confusion_mat(feedback_df):\n",
    "    cm_ord = confusion_matrix(feedback_df['label'], feedback_df['Recommended_Label'])\n",
    "    cls = model.labels_.inverse_transform(np.arange(len(label_series.unique().tolist()))).tolist()\n",
    "    Other_index = cls.index(\"Other\")\n",
    "    df_ord = pd.DataFrame(cm_ord,columns=cls)\n",
    "    df_ord[\"-Other-\"]=df_ord[\"Other\"]\n",
    "    df_ord = df_ord.drop('Other', 1)\n",
    "    cm_ord = df_ord.values.T\n",
    "    df_ord = pd.DataFrame(cm_ord,columns=cls)\n",
    "    df_ord[\"-Other-\"]=df_ord[\"Other\"]\n",
    "    df_ord = df_ord.drop('Other', 1)\n",
    "    cm_ord = df_ord.values.T\n",
    "    cls = df_ord.columns.values\n",
    "    return cm_ord, cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export Confusion Matrix to a .jpg file\n",
    "cm_plot_success=True\n",
    "try:\n",
    "    if not DEPLOY_MODEL:\n",
    "        plt.clf()\n",
    "        try:\n",
    "            #Try shifting Other to the end, if not - continue as printed from training model\n",
    "            cm, cls = move_Other_to_end_of_confusion_mat(feedback_df)\n",
    "        except:    \n",
    "            cm = confusion_matrix(feedback_df['label'], feedback_df['Recommended_Label'])\n",
    "            cls=model.labels_.inverse_transform(np.arange(len(label_series.unique().tolist())))\n",
    "        cm =np.ma.masked_where(np.diag(np.ones(cm.shape[0]))==1, cm) #The diaginal is masked in order to show it in green\n",
    "        try:\n",
    "            cm_plot = plot_confusion_matrix(cm, cm.data,classes=cls)\n",
    "        except:\n",
    "            #In case where ther's only one column in confudion matrix\n",
    "            cm_plot = plot_confusion_matrix(cm.data, cm.data,classes=cls)\n",
    "        \n",
    "        cm_plot.savefig(Amily_Path+\"Images/confusion_mat.jpg\")\n",
    "except:\n",
    "    logging.error('Confusion Matrix .PNG file was not created successfully')\n",
    "    cm_plot_success=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Precision/Recall Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate a data frame from the prescosion/recall report\n",
    "def report_to_df(report):\n",
    "    #This inner function was taken from a blog - not sure it's the most efficient\n",
    "    def report_to_table(report):\n",
    "        report = report.splitlines()\n",
    "        #print(report)\n",
    "        res = []\n",
    "        res.append(['']+report[0].split())\n",
    "        for row in report[2:-2]:\n",
    "            res.append(row.split())\n",
    "        lr = report[-1].split()\n",
    "        #print(lr)\n",
    "        #print([' '.join(lr[:3])]+lr[3:])\n",
    "        #res.append([' '.join(lr[:3])]+lr[3:])\n",
    "        res.append(lr)\n",
    "        return np.array(res)\n",
    "    table_arr=report_to_table(re.sub(\"(?<=\\S) (?=\\S)\",\"_\",report).replace(\"avg_/_total\",\"AVERAGE/SUMMARY\"))\n",
    "    #print(report)\n",
    "    stat_list=[]\n",
    "    for l in table_arr:\n",
    "        stat_list.append(l)\n",
    "    stat_list[0][0]='label'\n",
    "    #print(stat_list)\n",
    "    stats_df=pd.DataFrame(stat_list[1:], \n",
    "             columns=stat_list[0]\n",
    "            )\n",
    "    stats_df['label']=stats_df['label'].str.replace('_',' ')\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate a Precision-Recall Report\n",
    "def precision_recall_report(stats):\n",
    "    stat = report_to_df(stats)[['label','precision','recall','support']].set_index('label')\n",
    "    stat = stat.rename(columns={'support': '#Labeled Tickets', 'precision': 'Precision','recall':'Recall'})\n",
    "    \n",
    "    #Try removing \"Other\" to end of report\n",
    "    def move_label_to_end_of_report(report,label):\n",
    "        report_transposed = report.T\n",
    "        report_transposed['-'+label+\"-\"]=report_transposed[label]\n",
    "        report_transposed = report_transposed.drop(label, 1)\n",
    "        report_ordered = report_transposed.T\n",
    "        return report_ordered\n",
    "    try:\n",
    "        stat = move_label_to_end_of_report(stat,'Other')\n",
    "        stat = move_label_to_end_of_report(stat,'AVERAGE/SUMMARY')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stat['Precision'] = 1-stat['Precision'].astype(np.float)\n",
    "    stat['Recall'] = 1-stat['Recall'].astype(np.float)\n",
    "    stat['Precision']=stat['Precision'].apply('{:.0%}'.format)\n",
    "    stat['Recall']=stat['Recall'].apply('{:.0%}'.format)\n",
    "    stat = stat.rename(columns={'Precision': 'Error', 'Recall': 'Missed'})\n",
    "\n",
    "    plt.clf()\n",
    "    from pandas.tools.plotting import table\n",
    "    max_length_of_label = len(max(stat.index.tolist(), key=len))\n",
    "    #fig, ax = plt.subplots(figsize=(8.5, stat.shape[0]*0.32)) # set size frame\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8.5, stat.shape[0]*0.5)) # set size frame\n",
    "    \n",
    "    ax.xaxis.set_visible(False)  # hide the x axis\n",
    "    ax.yaxis.set_visible(False)  # hide the y axis\n",
    "    ax.set_frame_on(False)  # no visible frame, uncomment if size is ok\n",
    "    #tabla = table(ax, stat, loc='upper right', colWidths=[0.12,0.12,0.18])  # where df is your data frame\n",
    "    \n",
    "    tabla = table(ax, stat, loc='upper right', colWidths=[0.12,0.12,0.18])  # where df is your data frame\n",
    "    \n",
    "    tabla.auto_set_font_size(False) # Activate set fontsize manually\n",
    "    tabla.set_fontsize(11) # if ++fontsize is necessary ++colWidths\n",
    "    tabla.scale(1.2, 1.2) # change size table\n",
    "\n",
    "    plt.figtext(.47, 0.9, \"Statistical Summary\", fontsize=10,fontweight='bold')\n",
    "    #plt.title('asdasdasd')\n",
    "    plt.figtext(0.01, 0.03, \"Error - % of tickets classified as the flow icorrectly by atomIQ Ticketing\\nMissed - % of tickets labeld by the user to the flow and were not classified as the flow by atomIQ Ticketing\", fontsize=9)\n",
    "    plt.figtext(0.85,0.01,str(request_id), fontsize=8, color='#ccbbbb')\n",
    "    \n",
    "    training_type={True:'Quick Training',False:'Full Training'}\n",
    "    plt.figtext(0.85,0.03,training_type[QUICK_TRAINING],fontsize=8,color='#ccbbbb')\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Export Precision-Recall report to a .jpg file\n",
    "pr_plot_success=True\n",
    "try:\n",
    "    if not DEPLOY_MODEL:\n",
    "        pr_plot = precision_recall_report(stats)\n",
    "        pr_plot.savefig(Amily_Path+'Images/pre_recall_report.jpg')\n",
    "except:\n",
    "    logging.error('Precision-Recall report .jpg file was not created successfully')\n",
    "    pr_plot_success=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append Images to one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appends images to one image\n",
    "def pil_grid(images, max_horiz=np.iinfo(int).max):\n",
    "    n_images = len(images)\n",
    "    n_horiz = min(n_images, max_horiz)\n",
    "    h_sizes, v_sizes = [0] * n_horiz, [0] * (n_images // n_horiz)\n",
    "    for i, im in enumerate(images):\n",
    "        h, v = i % n_horiz, i // n_horiz\n",
    "        h_sizes[h] = max(h_sizes[h], im.size[0])\n",
    "        v_sizes[v] = max(v_sizes[v], im.size[1])\n",
    "    h_sizes, v_sizes = np.cumsum([0] + h_sizes), np.cumsum([0] + v_sizes)\n",
    "    im_grid = Image.new('RGB', (h_sizes[-1], v_sizes[-1]), color='white')\n",
    "    for i, im in enumerate(images):\n",
    "        im_grid.paste(im, (h_sizes[i % n_horiz], v_sizes[i // n_horiz]))\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add pictures to user feedback report, append them to one image and export to directory\n",
    "if not DEPLOY_MODEL:\n",
    "    try:\n",
    "        list_im=[Amily_Path+\"Images/Fixed/atomiq_logo.png\"]\n",
    "        if cm_plot_success:\n",
    "            list_im.append(Amily_Path+\"Images/confusion_mat.jpg\")\n",
    "        else:\n",
    "            list_im.append(Amily_Path+\"Images/Fixed/CM-Error.PNG\")\n",
    "\n",
    "        if pr_plot_success:\n",
    "            list_im.append(Amily_Path+\"Images/pre_recall_report.jpg\")\n",
    "        else:\n",
    "            list_im.append(Amily_Path+\"Images/Fixed/PR-Error.PNG\")\n",
    "\n",
    "        imgs = [Image.open(i) for i in list_im]\n",
    "        im = pil_grid(imgs,1)\n",
    "        if not TEST_ENVIROMENT:\n",
    "            saved_path=Output_Path+account_name+'--'+request_id+'.png'\n",
    "            im.save(saved_path)\n",
    "        else:\n",
    "            saved_path=Amily_Path+'Unit-test/Output/Training_Report_'+account_name+'--'+request_id+'.png'\n",
    "            im.save(saved_path)\n",
    "        logging.info('User feedback report was genertaed successfully')\n",
    "    except:\n",
    "        logging.error('User feedback report was not genertaed successfully')\n",
    "        try:\n",
    "            #Send a textual file containing an error message\n",
    "            if not TEST_ENVIROMENT:\n",
    "                saved_path=Output_Path+'Training_Report_'+account_name+'--'+request_id+'.txt'\n",
    "            else:\n",
    "                saved_path=Amily_Path+'Unit-test/Output/Training_Report_'+account_name+'--'+request_id+'.txt'\n",
    "            with open(saved_path, 'w') as error_msg_file:\n",
    "                error_msg_file.write('An error occurred while generating the Auto labeling statistics report for '+account_name+' ['+strftime(\"%Y-%m-%d %H:%M\")+']')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        os.remove(Amily_Path+'Images/confusion_mat.jpg')\n",
    "        os.remove(Amily_Path+'Images/pre_recall_report.jpg')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exports Preicision-Recall Report to Excel File - Disabled\n",
    "'''\n",
    "try:\n",
    "    output_path = Amily_Path+\"Output/\"+account_name+\"_statistics.xlsx\"\n",
    "    report_to_df(stats).to_excel(output_path,index=None)\n",
    "    logging.info('%s was created succesfully'%output_path)\n",
    "    logging.info('-----------------SUCCSEFULLY FINIHED TRAINING PROCESS FOR %s-----------------'%account_name)\n",
    "except:\n",
    "    logging.info('%s was not created succesfully. OPERATION ABORTED'%output_path)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Ticket - For Internal Use Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def review_row(train_df, ticket_id, textual_field):\n",
    "    for idx, row in train_df.loc[train_df['ticket_id']==ticket_id].iterrows():\n",
    "        print(row[textual_field])\n",
    "        \n",
    "tid = 'INC000002294732'\n",
    "review_row(train_df, ticket_id=tid, textual_field='textual_field_2')\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and NLP feature extractor to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_ml_model_file(path, model):\n",
    "    filter_value_dict={1:\"ext\",0:\"int\"}\n",
    "    filter_value=filter_value_dict[filter_values[0]]\n",
    "\n",
    "    Classification_model_file_name = account_name.replace(\" \",\"_\")+\"_\"+filter_value+'_Classification_model.pkl\"\n",
    "    with open(path+Classification_model_file_name, 'wb') as class_pkl:\n",
    "        pickle.dump(model, class_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def save_nlp_model_file(path):\n",
    "    filter_value_dict={1:\"ext\",0:\"int\"}\n",
    "    filter_value=filter_value_dict[filter_values[0]]\n",
    "    \n",
    "    acc=account_name.replace(' ','_')\n",
    "\n",
    "    source_nlp_model_file = glob.glob(''.join([Amily_Path,'Generated_Pickles/','*',acc,'*',filter_value,'*.pkl']))[0]\n",
    "    dest_nlp_model_file=path+source_nlp_model_file[source_nlp_model_file.rfind('/')+1:]\n",
    "\n",
    "    copyfile(source_nlp_model_file, dest_nlp_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEPLOY_MODEL:\n",
    "    path = Amily_Path+\"Outbound_File_Transfer/\"\n",
    "    try:     \n",
    "        save_ml_model_file(path, model)\n",
    "        logging.info('ML Model pickle was generated successfully')\n",
    "    except:\n",
    "        logging.error('ML Model pickle was not generated successfully')\n",
    "    \n",
    "    try:     \n",
    "        save_nlp_model_file(path)\n",
    "        logging.info('NLP Model pickle was copied successfully')\n",
    "    except:\n",
    "        logging.error('NLP Model pickle was not copied successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Reply to UTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not DEPLOY_MODEL:\n",
    "    send_result_request(success=True, detailed_results_path=detailed_report_path, \n",
    "                    stats_report_path=saved_path, company=account_name, request_id=request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete training file from directory upon training completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not TEST_ENVIROMENT:\n",
    "    try:\n",
    "        archive_infile(infile_name)\n",
    "        logging.info('-----------------SUCCESSFULLY FINIHED TRAINING PROCESS FOR %s-----------------'%account_name)\n",
    "    except:\n",
    "        logging.error('Could not delete training file after completion of training')\n",
    "else:\n",
    "    logging.info('-----------------SUCCESSFULLY FINIHED TRAINING PROCESS FOR %s-----------------'%account_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
