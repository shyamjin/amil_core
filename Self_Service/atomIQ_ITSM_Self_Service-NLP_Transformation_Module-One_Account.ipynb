{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atomIQ Ticketing Self Service - NLP Transformation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE - MULTIPROCESSING WORKS ONLY ON LINUX ENVIROMENT, THIS CODE WILL FREEZE ON WINDOWS**\n",
    "\n",
    "Implements one transformation pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basic packages - found in the Anaconda release\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.externals import joblib\n",
    "import logging\n",
    "from time import time, strftime\n",
    "import re\n",
    "from scipy.sparse import vstack, hstack\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UAT=True #UAT indicator, Assign to False when deploying in prod\n",
    "\n",
    "#Identification of running enviroment\n",
    "import socket\n",
    "server=socket.gethostname()\n",
    "UAT=True\n",
    "if 'prd3' in server:\n",
    "    UAT=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Amily_Path=\"/prjvl01/amily/Self_Service/\"\n",
    "TEST_ENVIROMENT=True if sys.argv[1]=='-f' else False #Unit Test indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Endpoints for AO integration - Load from configuration file\n",
    "with open(Amily_Path+'Features/Configurations/ao_endpoints.json') as json_data:\n",
    "    endpoints = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stopwords - reading file and preparing the stopwords as a list\n",
    "\n",
    "stopwords_file = open(\"./Features/Configurations/Amily_stopwords.txt\", \"r\")\n",
    "stopwords_amily = stopwords_file.read().split('\\n')\n",
    "#print(stopwords_amily)\n",
    "\n",
    "# Apply preprocessing to the stopwords\n",
    "for idx,stopword in enumerate(stopwords_amily):\n",
    "    stopwords_amily[idx] = stopword.lower()\n",
    "#print(stopwords_amily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=Amily_Path+'Logs/self_service.log',\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(message)s', \n",
    "                    datefmt='%Y-%m-%d,%H:%M:%S',\n",
    "                    level=logging.DEBUG\n",
    "                   )\n",
    "logging.getLogger(\"requests\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def send_result_request(status, company='Unknown', request_id=0):\n",
    "    if TEST_ENVIROMENT:\n",
    "        return\n",
    "    \n",
    "    comapny_name=company.replace(\"_\",\" \")\n",
    "    \n",
    "    #Login request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/login'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/login'\n",
    "    url=endpoints[server]['login']\n",
    "    headers = {'Content-Type' : 'application/json'}\n",
    "    body=str('{\"username\":\"Amily\",\\n\"password\":\"12345678\"}')\n",
    "    login_request = requests.post(url\n",
    "                                  ,data=body\n",
    "                                  ,headers=headers\n",
    "                                  ,verify = False\n",
    "                                 )\n",
    "    try:\n",
    "        token=login_request.headers['Authentication-Token']\n",
    "    except:\n",
    "        logging.error('Failed to fetch AO Token for REST Service')\n",
    "        return\n",
    "\n",
    "    \n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/process/:Amdocs_Amily_Integration_Interface:Self_Service_Amily_To_UTS/execute?mode=sync'\n",
    "    url=endpoints[server]['ack']\n",
    "    headers = {'Content-Type' : 'application/json' ,'Authentication-Token': token}\n",
    "            \n",
    "    #Status update request\n",
    "    fp1=r\"\"\"{\"inputParameters\":[{\"name\":\"Inputs\",\"value\":\"{'Ack': [{'Field': [{'Status': '\"\"\"\n",
    "    fp2=r\"\"\"'},{'StatusDescription': 'Operation successful'},{'Company': '\"\"\"\n",
    "    fp3=r\"\"\"'},{'RequestID': '\"\"\"\n",
    "    fp4=r\"\"\"'}]}]}\"}]}\"\"\"\n",
    "    ack_dict=\"\".join([fp1,str(status), fp2, str(comapny_name),fp3,str(request_id),fp4])\n",
    "    result_request = requests.post(url \n",
    "                                   ,data=ack_dict\n",
    "                                   ,headers=headers\n",
    "                                   ,verify = False\n",
    "                                  )\n",
    "    #print(file_paths_dict)\n",
    "    if result_request.status_code!=200:\n",
    "        logging.error('Failed in sending status to AO for NLP Transformation')\n",
    "    else:\n",
    "        logging.info('Acknowledgement Reply from UTS for NLP Transformation - >'+result_request.text)\n",
    "        \n",
    "    \n",
    "    #Logout request\n",
    "    #if UAT:\n",
    "    #    url = 'https://gssuts-uat-ao2:48443/baocdp/rest/logout'\n",
    "    #else:\n",
    "    #    url = 'https://gssuts-ao2:28443/baocdp/rest/logout'\n",
    "    url=endpoints[server]['logout']\n",
    "    headers = {'Content-Type' : 'application/json','Authentication-Token': token}\n",
    "    logout_request = requests.post(url\n",
    "                                   ,headers=headers\n",
    "                                   ,verify = False\n",
    "                                  )\n",
    "    if logout_request.status_code!=200:\n",
    "        logging.error('Failed logging out from AO')\n",
    "        return\n",
    "    else:\n",
    "        logging.info('Successfully logged out from AO')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DB read\n",
    "def read_corpus(path, cols):\n",
    "    \n",
    "    full_df = pd.read_csv(path, delimiter='\\t', encoding=\"utf8\")\n",
    "    \n",
    "    #Set column names to predefined values\n",
    "    column_dict = {}\n",
    "    column_dict[cols['ticket_id']]='ticket_id'\n",
    "    for idx, textual_field in enumerate(cols['textual_fields']):\n",
    "        col_name=str(\"_\".join(['textual_field',str(idx+1)]))\n",
    "        column_dict[textual_field]=col_name\n",
    "    for idx, filter_field in enumerate(cols['filter_fields']):\n",
    "        col_name=str(\"_\".join(['filter_field',str(idx+1)]))\n",
    "        column_dict[filter_field]=col_name\n",
    "    #column_dict[cols['company_field']]='company'\n",
    "    \n",
    "    #print('size ->',full_df.shape)\n",
    "    #print(column_dict)\n",
    "    full_df.rename(columns=column_dict,inplace=True)\n",
    "        \n",
    "    #Drop NA only after filters were done and only for the relevant columns\n",
    "    column_list=[]\n",
    "    for key, value in column_dict.items():\n",
    "        column_list.append(value)\n",
    "    full_df=full_df[column_list]\n",
    "    \n",
    "    #full_df.dropna(inplace=True, subset=column_list)\n",
    "    for idx, textual_field in enumerate(cols['textual_fields']):\n",
    "        col_name=str(\"_\".join(['textual_field',str(idx+1)]))\n",
    "        full_df[col_name]=full_df[col_name].fillna(\" \")\n",
    "    \n",
    "    full_df = full_df.drop_duplicates(subset=['ticket_id']).sort_values(by=['ticket_id']).reset_index(drop=True) \n",
    "    #print(full_df.info())\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - FILE WILL BE RECEIVED FROM ITSM\n",
    "if TEST_ENVIROMENT:\n",
    "    #infile_name = \"./Unit-test/Data/Deploy Test--007.txt\"\n",
    "    infile_name = \"./Unit-test/Data/MetroPCS Communications, Inc.--000000000001521.txt\"\n",
    "else:\n",
    "    infile_name = str(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Archive files - copy to Archive Directory and remove from orignal directory\n",
    "def archive_infile(infile_name):\n",
    "    try:\n",
    "        file_name=infile_name[infile_name.rfind('/')+1:]\n",
    "        copyfile(infile_name, Amily_Path+\"Archive/Transformations/\"+file_name)\n",
    "        os.remove(infile_name)\n",
    "        logging.info('Transformation file was moved to Archive folder')\n",
    "    except:\n",
    "        logging.warning('Transformation file was not moved succesffuly to Archive folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "''' Workaround for the pos_tag error in NLTK 3.2 '''\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.load(AP_MODEL_LOC)\n",
    "pos_tag = tagger.tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.count=0\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        #stopwords taken from the file - new line Maya 15.5.2018\n",
    "        self.stopwords  = stopwords_amily\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.start_pat=re.compile(r\"base64,\")\n",
    "        self.end_pat=re.compile(r\"&gt;\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        workers = mp.Pool()\n",
    "        return workers.map(self.transform_doc,X)\n",
    "    \n",
    "    def transform_doc(self, doc):\n",
    "        return list(self.tokenize(doc))\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        self.count+=1 \n",
    "        \n",
    "        #print(self.count)\n",
    "        cleaned=self.zap_base64(document)\n",
    "        #print(\"length of cleaned is %d\"%(len(cleaned)))\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(cleaned):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            #sent=sent.replace('*', ' ')\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                \n",
    "                # Transform all numbers to generic _NUMBER token\n",
    "                if token.isdigit():\n",
    "                    token = \"_NUMBER\"\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                #lemma = token\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def zap_base64(self, s):\n",
    "        #find start and end labels\n",
    "        start_matches=[m.start() for m in re.finditer(self.start_pat, s)]\n",
    "        if not start_matches:\n",
    "            return s\n",
    "        #print(\"=====\")\n",
    "        end_matches=[m.start() for m in re.finditer(self.end_pat, s)]\n",
    "        if len(end_matches)!=len(start_matches): #In case where the end char of the base64 encoding is not found\n",
    "            return s[:start_matches[0]]\n",
    "        try:\n",
    "            end_matches_ser=pd.Series(end_matches)\n",
    "            pair_list=[]\n",
    "            for st in start_matches:\n",
    "                end_loc=end_matches_ser.searchsorted(st)[0]\n",
    "                pair_list.append((st, end_matches_ser.loc[end_loc]))\n",
    "            #print(pair_list)\n",
    "            start_list=[0]\n",
    "            end_list=[]\n",
    "            for st, en in pair_list:\n",
    "                start_list.append(en+4)\n",
    "                end_list.append(st)\n",
    "            l=len(s)\n",
    "            end_list.append(l)\n",
    "            #for sp, ep in zip(start_list, end_list):\n",
    "                #print(sp, ep)\n",
    "            res=' '.join(s[sp:ep] for sp, ep in zip(start_list, end_list))\n",
    "        except:\n",
    "            logging.error(\"Base64 removal error\")\n",
    "            return s\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pipeline for preproccesing of the text - tokenization, lemmatziation and vectorization\n",
    "def vectorizer_fit_transform(X):\n",
    "    transformed_textual_lengths = {}\n",
    "    \n",
    "    transformer_list=[]\n",
    "    transformer_weights={}\n",
    "    \n",
    "    for i in range(len(cols['textual_fields'])):\n",
    "        field_name=\"_\".join(['textual_field',str(i+1)])\n",
    "        \n",
    "        textual_field_transformer = Pipeline([\n",
    "                ('selector', ItemSelector(key=field_name)),\n",
    "                ('preprocessor', NLTKPreprocessor()),\n",
    "                ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, \n",
    "                lowercase=False, ngram_range=(1,2)\n",
    "                )),\n",
    "            ])\n",
    "        transformer_list.append((field_name,textual_field_transformer))\n",
    "        transformer_weights[field_name]=1\n",
    "        \n",
    "        \n",
    "    transformer=Pipeline([\n",
    "        ('union', FeatureUnion(transformer_list=transformer_list, transformer_weights=transformer_weights)\n",
    "        )])\n",
    "    train_feat=transformer.fit_transform(X)\n",
    "    \n",
    "    #Calculate textual fields features lengths\n",
    "    for t in transformer.named_steps['union'].transformer_list:\n",
    "        transformed_textual_lengths[t[0]]=len(t[1].named_steps['vectorizer'].get_feature_names())\n",
    "        \n",
    "    #return textual_field_transformers, transformed_textual_lengths, train_feat\n",
    "    return transformer, transformed_textual_lengths, train_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Features for Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INPUT PARAMETERS - DEFAULT PARAMETERS FOR UTS\n",
    "\n",
    "ticket_id_field = \"Incident Number\"\n",
    "textual_fields=[\"Description\",\"Detailed Decription\"]\n",
    "#filter_fields=[\"z1D TicketTypeExternal?\"]\n",
    "filter_fields=[\"Origin Type\"]\n",
    "\n",
    "filter_dict={filter_fields[0]:\"Origin Type\"} #A UTS configuration, as the name of the field may be different between the NLP transformation file and the file received by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ticket_id                                    textual_field_1  \\\n",
      "0  INC000002274214             Metro: Rejects in billing for cycle 07   \n",
      "1  INC000002274227          Metro:ARGLDCMPEXT@ENDDAY job long running   \n",
      "2  INC000002274247                  Metro:Ensemble Stuck Port AMD_CNT   \n",
      "3  INC000002274326      Metro : EquipmentServiceGetEsnStatusOperation   \n",
      "4  INC000002274337  Metro : MPCSMO DeviceMgmt_V2.validateDeviceOp ...   \n",
      "\n",
      "                                     textual_field_2  filter_field_1  \n",
      "0  : 565719996 20 BLPREP BAN 565719996 rejected. ...               0  \n",
      "1  : Metro:ARGLDCMPEXT@ENDDAY job long running Re...               0  \n",
      "2  : SiteScope Measurement Critical 1/7/18 12:17:...               0  \n",
      "3  : EquipmentServiceGetEsnStatusOperation Critic...               0  \n",
      "4  : MPCSMO DeviceMgmt_V2.validateDeviceOp - SOA ...               0  \n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "cols = {\"ticket_id\":ticket_id_field\n",
    "       ,\"textual_fields\":textual_fields\n",
    "       ,\"filter_fields\":filter_fields\n",
    "        #,\"company_field\":company_field\n",
    "       }\n",
    "try:\n",
    "    file_name = infile_name[infile_name.rfind(\"/\")+1:]\n",
    "    company = file_name[:file_name.find(\"-\")].replace(\" \",\"_\")\n",
    "    request_id = file_name[file_name.find(\"-\")+2:file_name.rfind(\".\")]\n",
    "    transformation_df = read_corpus(infile_name,cols)\n",
    "    if not TEST_ENVIROMENT:\n",
    "        logging.info('-----------------NLP TRANSFORMATION SESSION FOR ACCOUNT %s HAS STARTED-----------------'%company)\n",
    "    else:\n",
    "        print(transformation_df.head())\n",
    "    send_result_request(status=\"InProgress\", company=company, request_id=request_id)\n",
    "except:\n",
    "    logging.error('Unable to load file %s for NLP Transformation'%infile_name)\n",
    "    try:\n",
    "        send_result_request(status=\"Failure\", company=company, request_id=request_id)\n",
    "    except:\n",
    "        send_result_request(status=\"Failure\")\n",
    "    #if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Specific implementation for the current UTS-driven Amily web service\n",
    "def align_pickle_to_uts(transformer):\n",
    "    textual_fields_in_web_service=[\"summary\",\"description\"]\n",
    "    for i, t in enumerate(transformer.named_steps['union'].transformer_list):\n",
    "        t[1].named_steps['selector'].key=textual_fields_in_web_service[i]\n",
    "    transformer.named_steps['union'].transformer_weights={'summary':1,'description':1}\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting and transforming records for Origin Type-0\n",
      "Finished transforming 4313 tickets of MetroPCS_Communications,_Inc. with Origin Type-0 in 2.00 [min.]\n",
      "fitting and transforming records for Origin Type-1\n",
      "Finished transforming 1142 tickets of MetroPCS_Communications,_Inc. with Origin Type-1 in 0.27 [min.]\n"
     ]
    }
   ],
   "source": [
    "#Generate NLP features for filter field value\n",
    "try:\n",
    "    for i in range(len(filter_fields)):\n",
    "        filter_field=\"_\".join([\"filter_field\",str(i+1)])\n",
    "        for filter_value in transformation_df[filter_field].unique().tolist():\n",
    "            start_time=time()\n",
    "            train_df=transformation_df.loc[transformation_df[filter_field]==filter_value]\n",
    "\n",
    "            verbose=True if TEST_ENVIROMENT else False\n",
    "            if verbose: print(\"fitting and transforming records for %s-%s\"%(str(filter_dict[filter_fields[i]]),filter_value))\n",
    "                \n",
    "            textual_field_transformer, transformed_textual_lengths, train_feat = vectorizer_fit_transform(train_df)\n",
    "            message = str(\"Finished transforming %d tickets of %s with %s-%s in %1.2f [min.]\"%(train_df.shape[0],company,str(filter_dict[filter_fields[i]]),filter_value,(time()-start_time)/60))\n",
    "            if not TEST_ENVIROMENT:\n",
    "                logging.info(message)\n",
    "            else:\n",
    "                print(message)\n",
    "\n",
    "            #Save compressed feature array to disk\n",
    "            file_prefix = \"_\".join([company.replace(\" \",\"_\"),str(filter_dict[filter_fields[i]])+'-'+str(filter_value)]) \n",
    "            output_file_name=\".\".join([file_prefix,'npz'])\n",
    "            np.savez_compressed(Amily_Path+'Features/'+output_file_name, train_feat)\n",
    "\n",
    "            #Save transformed textual field limits and ticket ids to disk - configuration file\n",
    "            account_dict={\"text_limits\":transformed_textual_lengths, \"ticket_ids\":train_df['ticket_id'].tolist()}\n",
    "            output_file_name=\".\".join([file_prefix,'json'])\n",
    "            with open(Amily_Path+'Features/'+output_file_name, 'w') as outfile:\n",
    "                json.dump(account_dict, outfile)\n",
    "            \n",
    "            #Save NLP Transformarion Pickle\n",
    "            textual_field_transformer=align_pickle_to_uts(textual_field_transformer) #SPECIFIC UTS IMPLEMENTATION\n",
    "            \n",
    "            filter_value_dict={1:\"ext\",0:\"int\"}\n",
    "            filter_value_for_file_name=filter_value_dict[filter_value]\n",
    "            Text_Preprocessor_file_name = \"\".join([company,'_',filter_value_for_file_name,\n",
    "                                                   '_NLP_Preprocessor.pkl'])\n",
    "            \n",
    "            with open(Amily_Path+'Generated_Pickles/'+Text_Preprocessor_file_name, 'wb') as NLP_pkl:\n",
    "                pickle.dump(textual_field_transformer, NLP_pkl)\n",
    "            \n",
    "    send_result_request(status=\"Success\", company=company, request_id=request_id)\n",
    "except:\n",
    "    send_result_request(status=\"Failure\", company=company, request_id=request_id)\n",
    "    logging.error('Could not complete NLP transformation process for %s'%company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #if not TEST_ENVIROMENT: archive_infile(infile_name)\n",
    "    logging.info('-----------------SUCCESSFULLY FINIHED NLP TRANSFORMATION PROCESS FOR %s-----------------'%company)\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
